{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tCoIR - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import textacy.keyterms\n",
    "import gui_utility\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER = '../../data'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n",
    "current_document_index = lambda: current_corpus_container().document_index\n",
    "\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "extract_args = dict(\n",
    "    args=dict(\n",
    "        ngrams=[1],\n",
    "        named_entities=False,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        min_freq=1,\n",
    "        include_pos=['NOUN'],\n",
    "        filter_stops=True,\n",
    "        filter_punct=True\n",
    "    ),\n",
    "    extra_stop_words=None,\n",
    "    substitutions=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0ff89038e84a34b4f82c1022e78737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, container=container)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 08:05:13,262 : INFO : Builiding vocabulary...\n",
      "2019-02-24 08:05:13,264 : INFO : Vocabulary of size 7 built from 8 terms.\n",
      "2019-02-24 08:05:13,266 : INFO : ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "2019-02-24 08:05:13,268 : INFO : ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "2019-02-24 08:05:13,269 : INFO : ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "2019-02-24 08:05:13,271 : INFO : ['past', 'the', 'barn', 'fell', '.', None]\n",
      "2019-02-24 08:05:13,272 : INFO : ['the', 'barn', 'fell', '.', None, None]\n",
      "2019-02-24 08:05:13,273 : INFO : ['barn', 'fell', '.', None, None, None]\n",
      "2019-02-24 08:05:13,275 : INFO : ['fell', '.', None, None, None, None]\n",
      "2019-02-24 08:05:13,276 : INFO : ['.', None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run OK\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import scipy.sparse as sp\n",
    "import itertools\n",
    "\n",
    "        \n",
    "def sliding_window(seq, n):\n",
    "    \"Returns a sliding centered window of size +/-n from (of width 2 * n + 1) \"\n",
    "    y = [None] * n + list(seq) + [None] * n\n",
    "    for i in range(n, len(y)+n):\n",
    "        yield y[i-n:i+n+1]\n",
    "\n",
    "def sliding_window_it(it, n):\n",
    "    it = itertools.chain([None] * n, it, [None] * n * 2)\n",
    "    tail = tuple(itertools.islice(it, n))\n",
    "    head = tuple(itertools.islice(it, n+1))\n",
    "    for v in it:\n",
    "        yield list(tail + head + (v,))\n",
    "        tail = tail[1:] + (head[0],)\n",
    "        head = head[1:] + (v,)\n",
    "\n",
    "def sliding_window3(seq, n=2):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(itertools.islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "\n",
    "class HyperspaceAnalogueToLanguageVectorizer():\n",
    "    \n",
    "    def __init__(self, corpus=None, token2id=None):\n",
    "        \"\"\"\n",
    "        Build vocabulary and create P_ij term-term matrix and P_i term global occurence vector\n",
    "        \n",
    "        Parameter:\n",
    "            corpus Iterable[Iterable[str]]\n",
    "\n",
    "        \"\"\"\n",
    "        self.token2id = token2id\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        if corpus is not None and token2id is None:\n",
    "            self.token2id = self._build_vocabulary(corpus)\n",
    "        \n",
    "        self.p_ij = None\n",
    "        self.p_i = None\n",
    "\n",
    "        self._id2token = None\n",
    "        \n",
    "    def _build_vocabulary(self, corpus):\n",
    "        ''' Iterates corpus and add distict terms to vocabulary '''\n",
    "        logger.info('Builiding vocabulary...')\n",
    "        token2id = collections.defaultdict()\n",
    "        token2id.default_factory = token2id.__len__\n",
    "        term_count = 0\n",
    "        for doc in corpus:\n",
    "            for term in doc:\n",
    "                token2id[term]\n",
    "                term_count += 1\n",
    "        self.term_count = term_count\n",
    "        logger.info('Vocabulary of size {} built from {} terms.'.format(len(token2id), term_count))\n",
    "        return dict(token2id)\n",
    "    \n",
    "    @property\n",
    "    def id2token(self):\n",
    "        if self._id2token is None:\n",
    "            if self.token2id is not None:\n",
    "                self._id2token = { v:k for k,v in self.token2id.items() }\n",
    "        return self._id2token\n",
    "    \n",
    "    def sliding_window(self, seq, n):\n",
    "        it = itertools.chain(iter(seq), [None] * n)\n",
    "        memory = tuple(itertools.islice(it, n+1))\n",
    "        if len(memory) == n+1:\n",
    "            yield memory\n",
    "        for x in it:\n",
    "            memory = memory[1:] + (x,)\n",
    "            yield memory\n",
    "        \n",
    "    def fit(self, corpus=None, size=2, weighing=0):\n",
    "        \n",
    "        '''Trains HAL for a document. Note that sentence borders (for now) are ignored'''\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.corpus = corpus\n",
    "            self.token2id = self._build_vocabulary(corpus)\n",
    "            \n",
    "        assert self.token2id is not None, \"Fit with no vocabulary!\"\n",
    "        assert self.corpus is not None, \"Fit with no corpus!\"\n",
    "\n",
    "        p_ij = sp.lil_matrix ((len(self.token2id), len(self.token2id)), dtype=np.float64)\n",
    "        p_i = np.zeros(len(self.token2id), dtype=np.int32)\n",
    "        \n",
    "        for terms in corpus:\n",
    "            \n",
    "            id_terms = ( self.token2id[size] for size in terms)\n",
    "\n",
    "            for win in self.sliding_window(id_terms, size):\n",
    "                \n",
    "                logger.info([ self.id2token[x] if x is not None else None for x in win])\n",
    "                \n",
    "                if win[0] is None:\n",
    "                    continue\n",
    "                    \n",
    "                for x in win:\n",
    "                    if x is not None:\n",
    "                        p_i[x] += 1\n",
    "\n",
    "                for i in range(1, size+1):\n",
    "\n",
    "                    if win[i] is None:\n",
    "                        continue\n",
    "                        \n",
    "                    d = i # abs(n - i)\n",
    "                    if weighing == 0: #  linear i.e. adjacent equals window size, then decreasing by one\n",
    "                        w = size - d + 1\n",
    "                    elif weighing == 1: # f(d) = 1 / d\n",
    "                        w = 1.0 / d\n",
    "                    elif weighing == 2: # Constant value of 1\n",
    "                        w = 1\n",
    "\n",
    "                    #print('*', i, self.id2token[win[0]], self.id2token[win[i]], w, [ self.id2token[x] if x is not None else None for x in win])\n",
    "                    p_ij[win[0], win[i]] += w\n",
    "\n",
    "        self.p_i = p_i\n",
    "        self.p_ij = p_ij\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def to_df(self):\n",
    "        columns = [ self.id2token[i] for i in range(0,len(self.token2id))]\n",
    "        return pd.DataFrame(\n",
    "            data=self.p_ij.todense(),\n",
    "            index=list(columns),\n",
    "            columns=list(columns),\n",
    "            dtype=np.float64\n",
    "        ).T\n",
    "    \n",
    "    def cooccurence(self):\n",
    "        \n",
    "        coo = self.p_ij.tocoo(copy=False)\n",
    "        df_p_i = pd.DataFrame(self.p_i, columns=['p_i_count'])\n",
    "        df = pd.DataFrame({'x_id': coo.row, 'y_id': coo.col, 'p_xy': coo.data})[['x_id', 'y_id', 'p_xy']].sort_values(['x_id', 'y_id']).reset_index(drop=True)\n",
    "        df = df.assign(x_term=df.x_id.apply(lambda x: self.id2token[x]), y_term=df.y_id.apply(lambda x: self.id2token[x]))\n",
    "        df = df.merge(df_p_i, left_on='x_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_x'})\n",
    "        df = df.merge(df_p_i, left_on='y_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_y'})\n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'p_xy', 'p_x', 'p_y']]\n",
    "        df.p_xy = df.p_xy / self.term_count\n",
    "        df.p_x = df.p_x / self.term_count\n",
    "        df.p_y = df.p_y / self.term_count\n",
    "        \n",
    "        df = df.assign(score=df.p_xy / (df.p_x + df.p_y - df.p_xy))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "test_burgess_litmus_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 07:59:39,018 : INFO : Builiding vocabulary...\n",
      "2019-02-24 07:59:39,019 : INFO : Vocabulary of size 7 built from 8 terms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 1 the horse 5 ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "* 2 the raced 4 ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "* 3 the past 3 ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "* 4 the the 2 ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "* 5 the barn 1 ['the', 'horse', 'raced', 'past', 'the', 'barn']\n",
      "* 1 horse raced 5 ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "* 2 horse past 4 ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "* 3 horse the 3 ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "* 4 horse barn 2 ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "* 5 horse fell 1 ['horse', 'raced', 'past', 'the', 'barn', 'fell']\n",
      "* 1 raced past 5 ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "* 2 raced the 4 ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "* 3 raced barn 3 ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "* 4 raced fell 2 ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "* 5 raced . 1 ['raced', 'past', 'the', 'barn', 'fell', '.']\n",
      "* 1 past the 5 ['past', 'the', 'barn', 'fell', '.', None]\n",
      "* 2 past barn 4 ['past', 'the', 'barn', 'fell', '.', None]\n",
      "* 3 past fell 3 ['past', 'the', 'barn', 'fell', '.', None]\n",
      "* 4 past . 2 ['past', 'the', 'barn', 'fell', '.', None]\n",
      "* 1 the barn 5 ['the', 'barn', 'fell', '.', None, None]\n",
      "* 2 the fell 4 ['the', 'barn', 'fell', '.', None, None]\n",
      "* 3 the . 3 ['the', 'barn', 'fell', '.', None, None]\n",
      "* 1 barn fell 5 ['barn', 'fell', '.', None, None, None]\n",
      "* 2 barn . 4 ['barn', 'fell', '.', None, None, None]\n",
      "* 1 fell . 5 ['fell', '.', None, None, None, None]\n",
      "Test run OK\n"
     ]
    }
   ],
   "source": [
    "def test_burgess_litmus_test():\n",
    "    terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "    answer = {\n",
    "     'barn':  {'.': 4,  'barn': 0,  'fell': 5,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'fell':  {'.': 5,  'barn': 0,  'fell': 0,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'horse': {'.': 0,  'barn': 2,  'fell': 1,  'horse': 0,  'past': 4,  'raced': 5,  'the': 3},\n",
    "     'past':  {'.': 2,  'barn': 4,  'fell': 3,  'horse': 0,  'past': 0,  'raced': 0,  'the': 5},\n",
    "     'raced': {'.': 1,  'barn': 3,  'fell': 2,  'horse': 0,  'past': 5,  'raced': 0,  'the': 4},\n",
    "     'the':   {'.': 3,  'barn': 6,  'fell': 4,  'horse': 5,  'past': 3,  'raced': 4,  'the': 2}\n",
    "    }\n",
    "    df_answer = pd.DataFrame(answer).astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    #display(df_answer)\n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "    vectorizer.fit([terms], size=5)\n",
    "    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    assert df_imp.equals(df_answer), \"Test failed\"\n",
    "    #df_imp == df_answer\n",
    "    print('Test run OK')\n",
    "        \n",
    "\n",
    "test_burgess_litmus_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 07:15:03,129 : INFO : Builiding vocabulary...\n",
      "2019-02-24 07:15:03,132 : INFO : Vocabulary of size 115 built from 171 terms.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fbb288f2c0408ebca6c029e197deca",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [ current_corpus()[0] ]\n",
    "terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(corpus, extract_args) ]\n",
    "#terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "vectorizer.fit(terms)\n",
    "df = vectorizer.cooccurence()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23b9b038748483195eef6adb5e79162",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'horse', 'raced', 'past', 'the', 'barn'), ('horse', 'raced', 'past', 'the', 'barn', 'fell'), ('raced', 'past', 'the', 'barn', 'fell', '.'), ('past', 'the', 'barn', 'fell', '.', None), ('the', 'barn', 'fell', '.', None, None), ('barn', 'fell', '.', None, None, None), ('fell', '.', None, None, None, None), ('.', None, None, None, None, None)]\n",
      "[[None, None, 'the', 'horse', 'raced', 'past'], [None, 'the', 'horse', 'raced', 'past', 'the'], ['the', 'horse', 'raced', 'past', 'the', 'barn'], ['horse', 'raced', 'past', 'the', 'barn', 'fell'], ['raced', 'past', 'the', 'barn', 'fell', '.'], ['past', 'the', 'barn', 'fell', '.', None], ['the', 'barn', 'fell', '.', None, None], ['barn', 'fell', '.', None, None, None], ['fell', '.', None, None, None, None]]\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(seq, n):\n",
    "    \"Returns a sliding centered window of size +/-n from (of width 2 * n + 1) \"\n",
    "    y = [None] * n + list(seq) + [None] * n\n",
    "    #y = itertools.chain([pad_value] * pad_left, it, [pad_value] * n * 2)\n",
    "    for i in range(n, len(y)+n):\n",
    "        yield y[i-n:i+n+1]\n",
    "\n",
    "def sliding_window2(seq, n):\n",
    "    it = itertools.chain(iter(seq), [None] * n)\n",
    "    memory = tuple(itertools.islice(it, n+1))\n",
    "    if len(memory) == n+1:\n",
    "        yield memory\n",
    "    for x in it:\n",
    "        memory = memory[1:] + (x,)\n",
    "        yield memory\n",
    "                    \n",
    "terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "print(list(sliding_window2(terms, 5)))\n",
    "print(list(sliding_window_it(terms, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
