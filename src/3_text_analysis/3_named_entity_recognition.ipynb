{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os #, collections, zipfile\n",
    "#import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER, PATTERN = '../../data',  '*.txt'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    # FIXME VARYING ASPECTS: document_index = WTI_INDEX for tCoIR\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, document_index=None, container=container, compute_ner=True, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, document_index):\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options = list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='50%')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "    )\n",
    "\n",
    "    def display_document_entities(corpus, document_id):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:        \n",
    "            doc = textacy_utility.get_document_by_id(corpus, document_id)\n",
    "            displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right]),\n",
    "        widgets.VBox([gui.output], layout=widgets.Layout(margin_top='20px', height='600px',width='100%'))\n",
    "    ]))\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        document_id=gui.document_id\n",
    "    )\n",
    "    \n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_document_entities_gui(corpus, document_index=document_index)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(spacy_doc, include_types=None, drop_determiners=True):\n",
    "    \n",
    "    entities = (x for x in spacy_doc.ents if not x.text.isspace())\n",
    "    \n",
    "    if include_types is not None:\n",
    "        assert isinstance(include_types, (set, list, tuple))\n",
    "        entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiners is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "\n",
    "    for x in entities:\n",
    "        yield x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Extract Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file run_ner_places.py\n",
    "#from cytoolz import itertoolz\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import domain_logic_vatican as domain_logic\n",
    "import text_corpus\n",
    "import logging\n",
    "import string\n",
    "\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "logger = logging.getLogger('ner')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "NON_PRINT_CHARS = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "\n",
    "DEL_WS_CHARS = str.maketrans('', '', '\"\\n\\t')\n",
    "DEL_CRAP_CHARS = str.maketrans('', '', '\"\\n\\t?@')# + NON_PRINT_CHARS)\n",
    "#DEL_NON_PRINT = str.maketrans('', '', NON_PRINT_CHARS)\n",
    "\n",
    "#def get_doc_places(doc):\n",
    "#    return ( (w.text, w.lemma_, str(w.lemma_).translate(DEL_CRAP_CHARS), w[0].ent_type_)\n",
    "#                for w in doc.ents if w[0].ent_type_ in ['LOC', 'GPE'] and w.lemma_.strip() != '' )\n",
    "\n",
    "def get_doc_place_entities(spacy_doc, drop_determiner=True):\n",
    "    \n",
    "    DET = spacy.parts_of_speech.DET\n",
    "    PUNCT = spacy.parts_of_speech.PUNCT\n",
    "    \n",
    "    include_types = ['LOC', 'GPE']\n",
    "    \n",
    "    entities = spacy_doc.ents\n",
    "    entities = (x for x in entities if not x.text.isspace())\n",
    "    entities = (x for x in entities if x.lemma_.strip() != '')\n",
    "    entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiner is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "        \n",
    "    for x in entities:\n",
    "        text = x.text.translate(DEL_WS_CHARS)\n",
    "        lemma = x.lemma_.translate(DEL_WS_CHARS)\n",
    "        #tidy_lemma = lemma.translate(DEL_CRAP_CHARS).replace('  ', ' ')\n",
    "        tidy_lemma = lemma.replace('  ', ' ')\n",
    "        yield (text, lemma, tidy_lemma, x[0].ent_type_)\n",
    "        \n",
    "#def get_corpus_places(corpus):\n",
    "#    return itertoolz.chain.from_iterable(( get_doc_places(doc.spacy_doc) for doc in corpus ))\n",
    "\n",
    "def create_source_stream(source_path, lang, document_index=None):\n",
    "    reader = text_corpus.CompressedFileReader(source_path)\n",
    "    stream = domain_logic.get_document_stream(reader, lang, document_index=document_index)\n",
    "    return stream\n",
    "\n",
    "def create_nlp(model='en_core_web_sm', disable=None):\n",
    "    nlp = spacy.load(model, disable=disable)\n",
    "    nlp.tokenizer = textacy_utility.keep_hyphen_tokenizer(nlp)\n",
    "    return nlp\n",
    "\n",
    "source_paths = [ '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip', '../../data/francis_curated_20190326.txt_preprocessed.zip' ]\n",
    "\n",
    "model_name = 'en_core_web_lg'\n",
    "\n",
    "logger.info('Loading model {}'.format(model_name))\n",
    "nlp = create_nlp(model=model_name, disable=('parser', 'textcat'))\n",
    "\n",
    "for source_path in source_paths:\n",
    "    logger.info('Processing {}...'.format(source_path))\n",
    "    stream = create_source_stream(source_path, 'en')\n",
    "    file_counter = 0\n",
    "    places = []\n",
    "    for filename, text, _ in stream:\n",
    "        file_counter += 1\n",
    "        doc = nlp(text)\n",
    "        places.extend(list(get_doc_place_entities(doc)))\n",
    "        if file_counter % 100 == 0:\n",
    "            logger.info('Processed {} files...{} places found...'.format(file_counter, len(places)))\n",
    "            #break\n",
    "        doc = None\n",
    "        \n",
    "df = pd.DataFrame(places, columns=['text', 'lemma', 'tidy_lemma', 'ent_type'])\n",
    "df.to_csv('./NER_with_tagging_total.txt', sep='\\t')\n",
    "\n",
    "df_grouped = df.groupby(['tidy_lemma', 'ent_type']).size().reset_index()\n",
    "\n",
    "df_grouped.to_csv('./NER_with_tagging_total_tidy_lemma_grouped.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#[z for z in textacy.extract.named_entities(doc)]\n",
    "#[[ ent for ent in textacy.extract.named_entities(doc) ] for doc in corpus if len(doc.spacy_doc.ents or []) > 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python3 run_ner_places.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Display Named Entity Statistics<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_named_entity_data(corpus, document_index, drop_determiners=True, min_freq=1):\n",
    "    #textacy.extract.named_entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\n",
    "    data = [[\n",
    "        (doc.metadata['document_id'], ent[0].ent_type_, ent.text, ent.lemma_)\n",
    "             for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n",
    "                for doc in corpus\n",
    "    ]\n",
    "    data = utility.flatten(data)\n",
    "    df = pd.DataFrame(data, columns=['document_id', 'ent_type', 'text', 'lemma']).set_index('document_id')\n",
    "    df = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')\n",
    "    return df[df.year > 0][['pope', 'year', 'genre', 'ent_type', 'text', 'lemma', 'filename']].reset_index()\n",
    "\n",
    "def display_grouped_by_entities_gui(corpus, document_index):\n",
    "    \n",
    "    columns = compile_named_entity_data([corpus[0]], document_index).columns\n",
    "    \n",
    "    group_by_options = [ (x.title(), x) for x in columns if x not in [ 'ent_type', 'text', 'lemma', 'filename', 'index'] ]\n",
    "    group_by_values = [ x for _, x in group_by_options ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        group_by=widgets.SelectMultiple(description='Group by', options=group_by_options, value=group_by_values, rows=3, layout=widgets.Layout(width='180px')),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop DET',  tooltip='Drop_determiners`', icon='check'),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "    )\n",
    "        \n",
    "    def display_grouped_by_entities(corpus, group_by, drop_determiners, min_freq):\n",
    "        gui.output.clear_output()\n",
    "        named_entities = compile_named_entity_data(corpus, document_index, drop_determiners, min_freq)\n",
    "        with gui.output:\n",
    "            df = named_entities.groupby(list(group_by) + ['ent_type', 'lemma']).size().reset_index()\n",
    "            df = df.rename(columns={0:'Count'})\n",
    "            df = df.sort_values('Count', ascending=False)\n",
    "            display(df)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_grouped_by_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        #named_entities=widgets.fixed(named_entities),\n",
    "        group_by=gui.group_by,\n",
    "        drop_determiners=gui.drop_determiners,\n",
    "        min_freq=gui.min_freq\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.group_by, gui.drop_determiners, gui.min_freq]),\n",
    "        widgets.VBox([gui.output]),\n",
    "        itw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_grouped_by_entities_gui(corpus, document_index)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Stanford NER Tagger (CoreNLP)<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>PREPARE</span> Verify that Stanford CoreNLP is up and running<span style='color: green; float: right'>SKIP</span>\n",
    "Stanford CoreNLP server must be started as described in:  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "\n",
    "With docker:\n",
    "```bash\n",
    "docker pull frnkenstien/corenlp\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t frnkenstien/corenlp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford tagger is up and running!\n",
      " Result: Stony/ORGANIZATION Brook/ORGANIZATION University/ORGANIZATION in/O NY/STATE_OR_PROVINCE\n"
     ]
    }
   ],
   "source": [
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "try:\n",
    "    from nltk.parse import corenlp\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=STANFORD_CORE_NLP_URL, encoding='utf8', tagtype='ner')\n",
    "    input_tokens = 'Stony Brook University in NY'.split()\n",
    "    tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "    print('Stanford tagger is up and running!')\n",
    "    print(' Result: ' + ' '.join([ x + '/' + y for x,y in tagged_output]))\n",
    "except: # (ConnectionError, ConnectionRefusedError):\n",
    "    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050807.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050814.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050815.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050821-20th-wyd.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050828.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050904.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050911.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050918.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20050925.txt\n",
      "benedict-xvi_en_angelus_2005_hf-ben-xvi-ang-20051002.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 14:02:02,585 : INFO : Processed 10 files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 19, 'Cologne', 'CITY'), (0, 1070, 'Bethlehem', 'CITY'), (0, 1308, 'Mount', 'LOCATION'), (0, 1309, 'Tabor', 'CITY'), (0, 1823, 'Cologne', 'CITY'), (0, 2277, 'Southern', 'LOCATION'), (0, 2278, 'Italy', 'COUNTRY'), (1, 1166, 'Rome', 'CITY'), (1, 1353, 'Europe', 'LOCATION'), (1, 1390, 'Auschwitz', 'LOCATION'), (1, 1923, 'Cologne', 'CITY'), (1, 2357, 'Rome', 'CITY'), (1, 2541, 'Cologne', 'CITY'), (1, 2543, 'Germany', 'COUNTRY'), (2, 1323, 'Cologne', 'CITY'), (2, 2118, 'Cyprus', 'COUNTRY'), (3, 948, 'Cologne', 'CITY'), (3, 1704, 'Sydney', 'CITY'), (3, 1706, 'Australia', 'COUNTRY'), (3, 2454, 'Cologne', 'CITY'), (3, 3535, 'Asia', 'CITY'), (3, 3776, 'Africa', 'LOCATION'), (4, 9, 'Cologne', 'CITY'), (4, 339, 'Germany', 'COUNTRY'), (4, 344, 'Italy', 'COUNTRY'), (4, 580, 'Cologne', 'CITY'), (4, 2045, 'St_Augustine', 'LOCATION'), (4, 3125, 'Castel_Gandolfo', 'LOCATION'), (4, 3129, 'Rome', 'CITY'), (5, 101, 'Vatican', 'LOCATION'), (5, 577, 'Mass', 'STATE_OR_PROVINCE'), (5, 870, 'Mass', 'STATE_OR_PROVINCE'), (5, 1897, 'Marienfeld', 'LOCATION'), (5, 2187, 'Cologne', 'CITY'), (5, 2189, 'Bonn', 'CITY'), (5, 2191, 'Düsseldorf', 'LOCATION'), (5, 3190, 'United_States_of_America', 'COUNTRY'), (5, 3195, 'New_Orleans', 'CITY'), (5, 4203, 'United_States', 'COUNTRY'), (6, 358, 'Mass', 'STATE_OR_PROVINCE'), (6, 433, 'Mass', 'STATE_OR_PROVINCE'), (6, 468, 'Golgotha', 'LOCATION'), (6, 1367, 'Mass', 'STATE_OR_PROVINCE'), (6, 2339, 'Mass', 'STATE_OR_PROVINCE'), (6, 2717, 'New_York', 'STATE_OR_PROVINCE'), (7, 2063, 'Padre', 'LOCATION'), (7, 2064, 'Pio', 'CITY'), (7, 2188, 'Mass', 'STATE_OR_PROVINCE'), (7, 2194, 'Calvary', 'LOCATION'), (7, 2424, 'Eucharist', 'LOCATION'), (7, 3235, 'Castel_Gandolfo', 'LOCATION'), (7, 3240, 'Rome', 'CITY'), (8, 14, 'Castel_Gandolfo', 'LOCATION'), (8, 898, 'Eucharist', 'LOCATION'), (8, 1344, 'Upper_Room', 'LOCATION'), (8, 2747, 'Annunciation', 'LOCATION'), (8, 3657, 'United_States', 'COUNTRY'), (9, 12, 'St_Peter', 'LOCATION'), (9, 1979, 'Far_East', 'LOCATION'), (9, 1996, 'Lisieux', 'CITY')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>entity</th>\n",
       "      <th>ent_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1070</td>\n",
       "      <td>Bethlehem</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Mount</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1309</td>\n",
       "      <td>Tabor</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1823</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2277</td>\n",
       "      <td>Southern</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2278</td>\n",
       "      <td>Italy</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1166</td>\n",
       "      <td>Rome</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1353</td>\n",
       "      <td>Europe</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1390</td>\n",
       "      <td>Auschwitz</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1923</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2357</td>\n",
       "      <td>Rome</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2541</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2543</td>\n",
       "      <td>Germany</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1323</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2118</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>948</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1704</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1706</td>\n",
       "      <td>Australia</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>2454</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3535</td>\n",
       "      <td>Asia</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>3776</td>\n",
       "      <td>Africa</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>339</td>\n",
       "      <td>Germany</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>344</td>\n",
       "      <td>Italy</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>580</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>2045</td>\n",
       "      <td>St_Augustine</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>3125</td>\n",
       "      <td>Castel_Gandolfo</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>3129</td>\n",
       "      <td>Rome</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>Vatican</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>577</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>870</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>1897</td>\n",
       "      <td>Marienfeld</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>2187</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>2189</td>\n",
       "      <td>Bonn</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>2191</td>\n",
       "      <td>Düsseldorf</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>3190</td>\n",
       "      <td>United_States_of_America</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>3195</td>\n",
       "      <td>New_Orleans</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>4203</td>\n",
       "      <td>United_States</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6</td>\n",
       "      <td>358</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6</td>\n",
       "      <td>433</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6</td>\n",
       "      <td>468</td>\n",
       "      <td>Golgotha</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6</td>\n",
       "      <td>1367</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6</td>\n",
       "      <td>2339</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6</td>\n",
       "      <td>2717</td>\n",
       "      <td>New_York</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7</td>\n",
       "      <td>2063</td>\n",
       "      <td>Padre</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7</td>\n",
       "      <td>2064</td>\n",
       "      <td>Pio</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7</td>\n",
       "      <td>2188</td>\n",
       "      <td>Mass</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>7</td>\n",
       "      <td>2194</td>\n",
       "      <td>Calvary</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7</td>\n",
       "      <td>2424</td>\n",
       "      <td>Eucharist</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7</td>\n",
       "      <td>3235</td>\n",
       "      <td>Castel_Gandolfo</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7</td>\n",
       "      <td>3240</td>\n",
       "      <td>Rome</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>Castel_Gandolfo</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>898</td>\n",
       "      <td>Eucharist</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8</td>\n",
       "      <td>1344</td>\n",
       "      <td>Upper_Room</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>8</td>\n",
       "      <td>2747</td>\n",
       "      <td>Annunciation</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8</td>\n",
       "      <td>3657</td>\n",
       "      <td>United_States</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>St_Peter</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9</td>\n",
       "      <td>1979</td>\n",
       "      <td>Far_East</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>9</td>\n",
       "      <td>1996</td>\n",
       "      <td>Lisieux</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id   pos                    entity           ent_type\n",
       "id                                                           \n",
       "0        0    19                   Cologne               CITY\n",
       "1        0  1070                 Bethlehem               CITY\n",
       "2        0  1308                     Mount           LOCATION\n",
       "3        0  1309                     Tabor               CITY\n",
       "4        0  1823                   Cologne               CITY\n",
       "5        0  2277                  Southern           LOCATION\n",
       "6        0  2278                     Italy            COUNTRY\n",
       "7        1  1166                      Rome               CITY\n",
       "8        1  1353                    Europe           LOCATION\n",
       "9        1  1390                 Auschwitz           LOCATION\n",
       "10       1  1923                   Cologne               CITY\n",
       "11       1  2357                      Rome               CITY\n",
       "12       1  2541                   Cologne               CITY\n",
       "13       1  2543                   Germany            COUNTRY\n",
       "14       2  1323                   Cologne               CITY\n",
       "15       2  2118                    Cyprus            COUNTRY\n",
       "16       3   948                   Cologne               CITY\n",
       "17       3  1704                    Sydney               CITY\n",
       "18       3  1706                 Australia            COUNTRY\n",
       "19       3  2454                   Cologne               CITY\n",
       "20       3  3535                      Asia               CITY\n",
       "21       3  3776                    Africa           LOCATION\n",
       "22       4     9                   Cologne               CITY\n",
       "23       4   339                   Germany            COUNTRY\n",
       "24       4   344                     Italy            COUNTRY\n",
       "25       4   580                   Cologne               CITY\n",
       "26       4  2045              St_Augustine           LOCATION\n",
       "27       4  3125           Castel_Gandolfo           LOCATION\n",
       "28       4  3129                      Rome               CITY\n",
       "29       5   101                   Vatican           LOCATION\n",
       "30       5   577                      Mass  STATE_OR_PROVINCE\n",
       "31       5   870                      Mass  STATE_OR_PROVINCE\n",
       "32       5  1897                Marienfeld           LOCATION\n",
       "33       5  2187                   Cologne               CITY\n",
       "34       5  2189                      Bonn               CITY\n",
       "35       5  2191                Düsseldorf           LOCATION\n",
       "36       5  3190  United_States_of_America            COUNTRY\n",
       "37       5  3195               New_Orleans               CITY\n",
       "38       5  4203             United_States            COUNTRY\n",
       "39       6   358                      Mass  STATE_OR_PROVINCE\n",
       "40       6   433                      Mass  STATE_OR_PROVINCE\n",
       "41       6   468                  Golgotha           LOCATION\n",
       "42       6  1367                      Mass  STATE_OR_PROVINCE\n",
       "43       6  2339                      Mass  STATE_OR_PROVINCE\n",
       "44       6  2717                  New_York  STATE_OR_PROVINCE\n",
       "45       7  2063                     Padre           LOCATION\n",
       "46       7  2064                       Pio               CITY\n",
       "47       7  2188                      Mass  STATE_OR_PROVINCE\n",
       "48       7  2194                   Calvary           LOCATION\n",
       "49       7  2424                 Eucharist           LOCATION\n",
       "50       7  3235           Castel_Gandolfo           LOCATION\n",
       "51       7  3240                      Rome               CITY\n",
       "52       8    14           Castel_Gandolfo           LOCATION\n",
       "53       8   898                 Eucharist           LOCATION\n",
       "54       8  1344                Upper_Room           LOCATION\n",
       "55       8  2747              Annunciation           LOCATION\n",
       "56       8  3657             United_States            COUNTRY\n",
       "57       9    12                  St_Peter           LOCATION\n",
       "58       9  1979                  Far_East           LOCATION\n",
       "59       9  1996                   Lisieux               CITY"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 14:02:02,626 : INFO : Result stored in ner_benedict-xvi_curated_20190326.txt_20190319140202.txt\n"
     ]
    }
   ],
   "source": [
    "%%file run_stanford_ner.py\n",
    "import os, sys, time\n",
    "import glob\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import text_corpus\n",
    "import domain_logic_vatican as domain_logic\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.widgets_config as widgets_config\n",
    "import common.utility as utility\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "from nltk.parse import corenlp\n",
    "\n",
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "def merge_entities(entities):\n",
    "    n_entities = len(entities)\n",
    "    if n_entities <= 1:\n",
    "        return entities\n",
    "    merged = entities[:1]\n",
    "    for doc_id, i_n, w_n, t_n in entities[1:]:\n",
    "        doc_id_p, i_p, w_p, t_p = merged[-1]\n",
    "        if i_n == i_p + 1 and t_n == t_p:\n",
    "            merged[-1] = (doc_id, i_n, '_'.join([w_p, w_n]), t_p)\n",
    "        else:\n",
    "            merged.append((doc_id, i_n, w_n, t_n))\n",
    "    return merged\n",
    "\n",
    "def recognize_named_entities(tagger, doc_id, text, excludes=None, includes=None):\n",
    "    ''' excludes not used '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    start_index = 0\n",
    "    excludes = excludes or []\n",
    "    includes = includes or []\n",
    "    merged_ents = []\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged_data = tagger.tag(tokens)\n",
    "        ents = [ (doc_id, start_index + index, word, ent_type) for index, (word, ent_type) \\\n",
    "                in enumerate(tagged_data) if ent_type in includes ]\n",
    "        start_index += len(sentence)\n",
    "        merged_ents.extend(merge_entities(ents))\n",
    "    return merged_ents\n",
    "\n",
    "#\n",
    "#     if not os.path.isfile(targetfile) or overwrite:\n",
    "#         if os.path.isfile(targetfile):\n",
    "#             os.remove(targetfile)\n",
    "#         with zipfile.ZipFile(targetfile, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "#             pass\n",
    "#     else:\n",
    "        \n",
    "#         with zipfile.ZipFile(targetfile, \"r\") as z:\n",
    "#             zip_filenames = z.namelist()\n",
    "            \n",
    "#         pdf_filepaths = list(set(pdf_filepaths) - set(zip_filenames))\n",
    "    \n",
    "#     with zipfile.ZipFile(targetfile, \"a\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "#         z.writestr(text_filename, pdf_text)\n",
    "            \n",
    "def compute_stanford_ner(source_file, service_url=STANFORD_CORE_NLP_URL, excludes=None, includes=None):\n",
    "    \n",
    "    includes = includes or (\n",
    "        'LOCATION', 'CITY', 'STATE_OR_PROVINCE', 'COUNTRY'\n",
    "    )\n",
    "    \n",
    "    # For English, by default, this annotator recognizes named (PERSON, LOCATION, ORGANIZATION, MISC),\n",
    "    # numerical (MONEY, NUMBER, ORDINAL, PERCENT), and temporal (DATE, TIME, DURATION, SET) entities (12 classes).\n",
    "    # Adding the regexner annotator and using the supplied RegexNER pattern files adds support for the fine-grained and additional entity \n",
    "    # classes EMAIL, URL, CITY, STATE_OR_PROVINCE, COUNTRY, NATIONALITY, RELIGION, (job) \n",
    "    # TITLE, IDEOLOGY, CRIMINAL_CHARGE, CAUSE_OF_DEATH (11 classes) for a total of 23 classes.\n",
    "\n",
    "    assert os.path.isfile(source_file), 'File missing!'\n",
    "    \n",
    "    tagger = corenlp.CoreNLPParser(url=service_url, encoding='utf8', tagtype='ner')\n",
    "    \n",
    "    reader = text_corpus.CompressedFileReader(source_file)\n",
    "    document_index = domain_logic.compile_documents_by_filename(reader.filenames)\n",
    "    stream = domain_logic.get_document_stream(reader, 'en', document_index=document_index)\n",
    "    \n",
    "    i = 0\n",
    "    ner_data = []\n",
    "    for filename, text, metadata in stream:\n",
    "        print(filename)\n",
    "        document_id = document_index.loc[document_index.filename == filename, 'document_id'].values[0]\n",
    "        ner = recognize_named_entities(tagger, document_id, text, excludes, includes)\n",
    "        ner_data.extend(ner)\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            logger.info('Processed {} files...'.format(i))\n",
    "            break\n",
    "        \n",
    "    return ner_data\n",
    "\n",
    "def compute_and_store_stanford_ner(source_file):\n",
    "    \n",
    "    ner_data = compute_stanford_ner(source_file=source_file)\n",
    "    print(ner_data)\n",
    "    df = pd.DataFrame(ner_data, columns=['doc_id', 'pos', 'entity', 'ent_type'])\n",
    "    df.index.name = 'id'\n",
    "\n",
    "    store_name = 'ner_{}_{}.txt'.format(\n",
    "        os.path.splitext(os.path.split(source_file)[1])[0],\n",
    "        time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "    )\n",
    "\n",
    "    df.to_csv(store_name, sep='\\t')\n",
    "    display(df)\n",
    "    logger.info('Result stored in %s', store_name)\n",
    "    return df\n",
    "        \n",
    "def display_stanford_ner_gui(data_folder):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus_files = sorted(glob.glob(os.path.join(data_folder, '*.txt.zip')))\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        source_path=widgets_config.dropdown(description='Corpus', options=corpus_files, value=corpus_files[-1], layout=lw('300px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px'))\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.source_path,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def compute_stanford_ner_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = compute_and_store_stanford_ner(gui.source_path.value)\n",
    "            display(df)\n",
    "    gui.compute.on_click(compute_stanford_ner_callback)\n",
    "\n",
    "\n",
    "#data_folder = '../../data'\n",
    "#display_stanford_ner_gui(data_folder)\n",
    "for source_file in [ '../../data/benedict-xvi_curated_20190326.txt.zip']: #, '../../data/francis_curated_20190326.txt.zip' ]:\n",
    "    compute_and_store_stanford_ner(source_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benedict-xvi_curated_20190326.txt_preprocessed', '.zip')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip'\n",
    "os.path.splitext(os.path.split(filename)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, codecs, time, re, collections, zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes=[ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data) if classes is None or wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i - 1][0] + 1 and entities[i][2] == entities[i - 1][2]:\n",
    "            entities[i - 1] = (entities[i - 1][0], entities[i - 1][1] + \" \" + entities[i][1], entities[i - 1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8', tagtype='ner')\n",
    "    return corenlp_tagger\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    corenlp_tokenizer = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8')\n",
    "    return corenlp_tokenizer\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, treaty_id, lang = extract_document_info(filename)\n",
    "    data = [ (document_name, treaty_id, lang, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x: ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "def recognize_entities(options):\n",
    "\n",
    "    corenlp_tokenizer = create_tokenizer(options)\n",
    "    corenlp_tagger = create_ner_tagger(options)\n",
    "    \n",
    "    outfile = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "    tags = [ 'NUMBER', 'LOCATION', 'DATE', 'MISC', 'ORGANIZATION', 'DURATION', 'SET', 'ORDINAL', 'PERSON' ]\n",
    "    \n",
    "    document_stream = treaty_corpus.get_document_stream(options['source_path'], options['language'], treaties)\n",
    "    for treaty_id, language, filename, content in document_stream:\n",
    "        print('treaty_id')\n",
    "        \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"../data/test_corpora_preprocessed.zip\",\n",
    "    'server_url': STANFORD_CORE_NLP_URL,\n",
    "    'output_folder': DATA_FOLDER,\n",
    "}\n",
    "\n",
    "recognize_entities(options)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
